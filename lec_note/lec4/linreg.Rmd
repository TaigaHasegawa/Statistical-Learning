---
title: Linear Regression and Model Selection
author: STAT 432
abstract: This is the supplementary `R` file for linear regression in the lecture note "LinearReg".
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(fig.align='center')
options(width = 1000)
```

## Basic Concepts

Suppose we collect a set of observations with design matrix $\mathbf{X}$ and outcome $\mathbf{y}$, linear regression estimate the coefficients through 

$$ \widehat{\boldsymbol \beta} = \underset{\boldsymbol \beta}{\arg\min} \big( \mathbf y - \mathbf{X} \boldsymbol \beta \big)^\text{T} \big( \mathbf y - \mathbf{X} \boldsymbol \beta \big) $$
This can be viewed as either a covex optimization problem or projections on the $n$ dimentional vector space. 

### Linear regression as an optimization

```{r fig.width=5, fig.height=5}
    # generate data for a simple linear regression 
    set.seed(20)
    n = 100
    x <- cbind(1, rnorm(n))
    y <- x %*% c(1, 0.5) + rnorm(n)
    
    # calculate the residual sum of squares for a grid of beta values
    rss <- function(b, x, y) sum((y - x %*% b)^2)
    b1 <- b2 <- seq(0, 2, length= 20)
    z = matrix(apply(expand.grid(b1, b2), 1, rss, x, y), 20, 20)
    
    # 3d plot for RSS
    par(mar = c(1,1,3,1))
    persp(b1, b2, z, xlab = "beta 1", ylab = "beta 2", zlab = "RSS",
          main="Residual Sum of Squares", col = "springgreen", shade = 0.6,
          theta = 30, phi = 5)
    
    # The solution can be solved by any optimization algorithm 
    optim(c(0, 0), rss, x = x, y = y)$par
```

### Linear regression as projections

Another view is through projections in vector space. Consider each column of $\mathbf{X}$ as a vector, and project $\mathbf{y}$ onto the column space of $\mathbf{X}$. The project is 

$$ \widehat{\mathbf{y}} = \mathbf{X} (\mathbf{X}^\text{T} \mathbf{X})^{-1}\mathbf{X}^\text{T} \mathbf{y} \doteq {\mathbf{H}} \mathbf{y}, $$
where $\mathbf{H}$ is a projection matrix. And the residuals are simply 

$$ \widehat{\mathbf{e}} = \mathbf{y} - \widehat{\mathbf{y}} = (\mathbf{I} - \mathbf{H}) \mathbf{y} $$
When the number of variables is large, inverting $\mathbf{X}^\text{T} \mathbf{X}$ is expansive. The `R` function `lm()` does not calculate the inverse directly. Instead, QR decomposition can be used. You can try a larger $n$ and $p$ to see a significant difference. This is only for demonstration. They are not required for our course. 

```{r}
    # generate 100 observations with 3 variables
    set.seed(1)
    n = 1000
    p = 500
    x = matrix(rnorm(n*p), n, p)
    X = cbind(1, x) # the design matrix, including 1 as the first column
    
    # define the true beta, the first entry is the intercept
    b = as.matrix(c(1, 1, 0.5, rep(0, p-2))) 
    
    # generate training y with Gaussian errors
    y = X %*% b + rnorm(n)
    
    # fit a linear regression model 
    lm.fit = lm(y ~ x)
    
    # look at the coefficients beta hat
    head(lm.fit$coef)
    
    # using normal equations by inverting the X'X matrix: b = (X'X)^-1 X'y 
    # however, this is very slow
    # check ?solve
    system.time({beta_hat = solve(t(X) %*% X) %*% t(X) %*% y})
    head(beta_hat)
    
    # you can avoid the inversion by specifying the linear equation system X'X b = X'y 
    system.time({beta_hat = solve(t(X) %*% X, t(X) %*% y)})    
    
    # A better approach is to use QR decomposition or the Cholesky decomposition 
    # The following codes are not necessarily efficient, they are only for demonstration purpose
    
    # QR decomposition
    # direct calling the qr.coef function
    system.time({beta_hat = qr.coef(qr(X), y)})
    
    # or 
    system.time({beta_hat = qr.solve(t(X) %*% X, t(X) %*% y)})
    
    # if you want to see what Q and R are
    QR = qr(X)
    Q = qr.Q(QR)
    R = qr.R(QR)
    
    # get inverse of R, you can check R %*% R_inv yourself
    # the backsolve/forwardsolve functions can be used to solve AX = b for upper/lower triangular matrix A 
    # ?backsolve
    R_inv = backsolve(R, diag(p+1), upper.tri = TRUE, transpose = FALSE)
    beta_hat = R_inv %*% t(Q) %*% y
    
    # Cholesky Decomposition 
    
    # the chol function gives upper triangular matrix
    # crossprod(X) = X'X
    system.time({
    R = chol(crossprod(X))
    w = backsolve(R, t(X) %*% y, upper.tri = TRUE, transpose = TRUE)
    beta_hat = backsolve(R, w, upper.tri = TRUE, transpose = FALSE)
    })
    
    # or equivalently 
    R = t(chol(crossprod(X)))
    w = forwardsolve(R, t(X) %*% y, upper.tri = FALSE, transpose = FALSE)
    beta_hat = forwardsolve(R, w, upper.tri = FALSE, transpose = TRUE) # the transpose = TRUE means that we are solving for R'b = w instead of Rb = w 
```

## Model Selection Criteria and Algorithm

### Example: `diabetes` dataset

We use the `diabetes` dataset from the `lars` package as a demonstration of model selection. 

```{r}
    library(lars)
    data(diabetes)
    diab = data.frame(cbind(diabetes$x, "Y" = diabetes$y))
    dim(diab)
    # A Brief Description of the Diabetes Data (Efron et al, 2004):
    # Ten baseline variables: age, sex, body mass index, average blood pressure, and six blood serum
    # measurements were obtained for each of n = 442 diabetes patients, as well as
    # the response of interest, a quantitative measure of disease progression one year after baseline 
    
    lmfit=lm(Y~., data=diab)
    
    # When we use normal distribution likelihood for the errors, there are 12 parameters
    # The function AIC() directly calculates the AIC score from a lm() fitted model 
    n = nrow(diab)
    p = 11

    # ?AIC
    AIC(lmfit) # a build-in function for calculating AIC using -2log likelihood
    n*log(sum(residuals(lmfit)^2/n)) + n + n*log(2*pi) + 2 + 2*p

    # In many standard R packages, the AIC is calculated by removing some constants from the likelihood 
    # We will use this value as the default
    ?extractAIC
    extractAIC(lmfit) # AIC for the full model
    RSS = sum(residuals(lmfit)^2)
    n*log(RSS/n) + 2*p

    # so the BIC for the full model is 
    extractAIC(lmfit, k = log(n))
    n*log(RSS/n) + log(n)*p
    
    # if we want to calculate Cp, use the formula
    RSS + 2*p*summary(lmfit)$sigma^2
    
    # however, the scale of this is usually very large, we may consider the following version
    RSS/summary(lmfit)$sigma^2 + 2*p - n
```

The `step()` function can be used to select the best model based on specified model selection criteria. 

```{r}
    # Model selection: stepwise algorithm 
    # ?step
    
    # this function shows every step during the model selection 
    step(lmfit, direction="both", k = 2)    # k = 2 (AIC) is default; 
    
    step(lmfit, direction="backward", trace=0) # trace=0 will not print intermediate results
    step(lm(Y~1, data=diab), scope=list(upper=lmfit, lower=~1), direction="forward", trace=0)
    
    step(lmfit, direction="both", k=log(n), trace=0)  # BIC (the default value for k=2, which corresponds to AIC)
```

The `leaps` package will calculate the best model of each model size. Then we can add the penalties to the model fitting result and conclude the best model. 

```{r}
    ##########################################################################
    # Best subset model selection (Cp, AIC, and BIC): leaps 
    ##########################################################################
    library(leaps)
    
    # performs an exhaustive search over models, and gives back the best model 
    # (with low RSS) of each size.
    # the default maximum model size is nvmax=8
    
    RSSleaps=regsubsets(as.matrix(diab[,-11]),diab[,11])
    summary(RSSleaps, matrix=T)
    
    RSSleaps=regsubsets(as.matrix(diab[,-11]),diab[,11], nvmax=10)
    summary(RSSleaps,matrix=T)
    
    sumleaps=summary(RSSleaps,matrix=T)
    names(sumleaps)  # components returned by summary(RSSleaps)
    
    sumleaps$which
    msize=apply(sumleaps$which,1,sum)
    n=dim(diab)[1]
    p=dim(diab)[2]
    Cp = sumleaps$rss/(summary(lmfit)$sigma^2) + 2*msize - n;
    AIC = n*log(sumleaps$rss/n) + 2*msize;
    BIC = n*log(sumleaps$rss/n) + msize*log(n);
    
    cbind(Cp, sumleaps$cp)
    cbind(BIC, sumleaps$bic)  # It seems regsubsets uses a formula for BIC different from the one we used. 
    BIC-sumleaps$bic  # But the two just differ by a constant, so won't affect the model selection result. 
    n*log(sum((diab[,11] - mean(diab[,11]))^2/n)) # the difference is the score of an intercept model
    
    # Rescale Cp, AIC, BIC to (0,1).
    inrange <- function(x) { (x - min(x)) / (max(x) - min(x)) }
    
    Cp = sumleaps$cp; Cp = inrange(Cp);
    BIC = sumleaps$bic; BIC = inrange(BIC);
    AIC = n*log(sumleaps$rss/n) + 2*msize; AIC = inrange(AIC);
    
    plot(range(msize), c(0, 1.1), type="n", 
         xlab="Model Size (with Intercept)", ylab="Model Selection Criteria")
    points(msize, Cp, col="red", type="b")
    points(msize, AIC, col="blue", type="b")
    points(msize, BIC, col="black", type="b")
    legend("topright", lty=rep(1,3), col=c("red", "blue", "black"), legend=c("Cp", "AIC", "BIC"))
```
